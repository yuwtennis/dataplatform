{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/usr/local/creds/GoogleDev-f20867bbe55e.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=/usr/local/creds/GoogleDev-f20867bbe55e.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pipeline created.\n",
      "/opt/anaconda2/lib/python2.7/site-packages/apache_beam/io/gcp/gcsio.py:166: DeprecationWarning: object() takes no parameters\n",
      "  super(GcsIO, cls).__new__(cls, storage_client))\n",
      "INFO:root:Starting the size estimation of the input\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Finished the size estimation of the input at 1 files. Estimation took 0.438647985458 seconds\n",
      "INFO:root:Starting the size estimation of the input\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Finished the size estimation of the input at 1 files. Estimation took 0.390017986298 seconds\n",
      "INFO:root:Starting GCS upload to gs://myfirstbucket-1/staging/job-write-file.1527554868.768140/pipeline.pb...\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Completed GCS upload to gs://myfirstbucket-1/staging/job-write-file.1527554868.768140/pipeline.pb\n",
      "INFO:root:Staging the SDK tarball from PyPI to gs://myfirstbucket-1/staging/job-write-file.1527554868.768140/dataflow_python_sdk.tar\n",
      "INFO:root:Executing command: ['/opt/anaconda2/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmplpJFKS', 'google-cloud-dataflow==2.4.0', '--no-binary', ':all:', '--no-deps']\n",
      "INFO:root:file copy from /tmp/tmplpJFKS/google-cloud-dataflow-2.4.0.tar.gz to gs://myfirstbucket-1/staging/job-write-file.1527554868.768140/dataflow_python_sdk.tar.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Create job: <Job\n",
      " createTime: u'2018-05-29T00:48:00.116424Z'\n",
      " currentStateTime: u'1970-01-01T00:00:00Z'\n",
      " id: u'2018-05-28_17_47_58-13154540430729834146'\n",
      " location: u'asia-northeast1'\n",
      " name: u'job-write-file'\n",
      " projectId: u'elite-caster-125113'\n",
      " stageStates: []\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:root:Created job with id: [2018-05-28_17_47_58-13154540430729834146]\n",
      "INFO:root:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/asia-northeast1/jobs/2018-05-28_17_47_58-13154540430729834146?project=elite-caster-125113\n",
      "INFO:root:Job 2018-05-28_17_47_58-13154540430729834146 is in state JOB_STATE_PENDING\n",
      "INFO:root:2018-05-29T00:47:58.311Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2018-05-28_17_47_58-13154540430729834146. The number of workers will be between 1 and 1000.\n",
      "INFO:root:2018-05-29T00:47:58.316Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2018-05-28_17_47_58-13154540430729834146.\n",
      "INFO:root:2018-05-29T00:48:02.989Z: JOB_MESSAGE_DETAILED: Checking required Cloud APIs are enabled.\n",
      "INFO:root:2018-05-29T00:48:03.323Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:root:2018-05-29T00:48:04.768Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in asia-northeast1-b.\n",
      "INFO:root:2018-05-29T00:48:06.071Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:root:2018-05-29T00:48:06.079Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:root:2018-05-29T00:48:06.086Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:root:2018-05-29T00:48:06.093Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:root:2018-05-29T00:48:06.106Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:root:2018-05-29T00:48:06.117Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:root:2018-05-29T00:48:06.124Z: JOB_MESSAGE_DETAILED: Fusing consumer Print into ReadFromText/Read\n",
      "INFO:root:2018-05-29T00:48:06.130Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into ReadFromText/Read\n",
      "INFO:root:2018-05-29T00:48:06.136Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/GroupByKey/Reify into WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:root:2018-05-29T00:48:06.143Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/GroupByKey/Write into WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:root:2018-05-29T00:48:06.149Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/Pair into WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:root:2018-05-29T00:48:06.156Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into WriteToText/Write/WriteImpl/Pair\n",
      "INFO:root:2018-05-29T00:48:06.162Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/Extract into WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:root:2018-05-29T00:48:06.168Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:root:2018-05-29T00:48:06.175Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/InitializeWrite into WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:root:2018-05-29T00:48:06.182Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:root:2018-05-29T00:48:06.188Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:root:2018-05-29T00:48:06.194Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:root:2018-05-29T00:48:06.201Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:root:Job 2018-05-28_17_47_58-13154540430729834146 is in state JOB_STATE_RUNNING\n",
      "INFO:root:2018-05-29T00:48:06.242Z: JOB_MESSAGE_DEBUG: Executing wait step start15\n",
      "INFO:root:2018-05-29T00:48:06.259Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/DoOnce/Read+WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:root:2018-05-29T00:48:06.266Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:root:2018-05-29T00:48:06.277Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:root:2018-05-29T00:48:06.283Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-northeast1-b...\n",
      "INFO:root:2018-05-29T00:48:06.302Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:root:2018-05-29T00:48:15.948Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 0 based on the rate of progress in the currently running step(s).\n",
      "INFO:root:2018-05-29T00:48:27.704Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:root:2018-05-29T00:48:45.159Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:root:2018-05-29T00:49:33.676Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:root:2018-05-29T00:49:33.683Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:root:2018-05-29T00:49:33.699Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:root:2018-05-29T00:49:33.706Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:root:2018-05-29T00:49:33.858Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:root:2018-05-29T00:49:33.865Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:root:2018-05-29T00:49:33.872Z: JOB_MESSAGE_BASIC: Executing operation ReadFromText/Read+Print+WriteToText/Write/WriteImpl/WriteBundles/WriteBundles+WriteToText/Write/WriteImpl/Pair+WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+WriteToText/Write/WriteImpl/GroupByKey/Reify+WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:root:2018-05-29T00:49:41.119Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/GroupByKey/Close\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:2018-05-29T00:49:41.135Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/GroupByKey/Read+WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+WriteToText/Write/WriteImpl/Extract\n",
      "INFO:root:2018-05-29T00:49:52.036Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:root:2018-05-29T00:49:52.050Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:root:2018-05-29T00:49:52.213Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:root:2018-05-29T00:49:52.227Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:root:2018-05-29T00:49:59.302Z: JOB_MESSAGE_DEBUG: Executing success step success13\n",
      "INFO:root:2018-05-29T00:50:00.101Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:root:2018-05-29T00:50:00.326Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:root:2018-05-29T00:50:00.334Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:root:2018-05-29T00:50:45.272Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:root:2018-05-29T00:50:45.280Z: JOB_MESSAGE_DETAILED: Autoscaling: Would further reduce the number of workers but reached the minimum number allowed for the job.\n",
      "INFO:root:2018-05-29T00:50:45.298Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:root:Job 2018-05-28_17_47_58-13154540430729834146 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "        \n",
    "def print_info(s):\n",
    "    print s\n",
    "    \n",
    "def run_pipeline():\n",
    "    # Prepare necessary options for beam pipeline\n",
    "    # https://beam.apache.org/documentation/runners/capability-matrix/\n",
    "    #\n",
    "    # Pipline parameters in detail\n",
    "    # https://cloud.google.com/dataflow/pipelines/specifying-exec-params\n",
    "    \n",
    "    # Specify the which runner to use\n",
    "    # RUNNER          = \"DirectRunner\"\n",
    "    RUNNER            = \"DataflowRunner\"\n",
    "    \n",
    "    # Path to store the code package used by worker in during staging\n",
    "    STAGING_LOCATION  = \"gs://myfirstbucket-1/staging\"\n",
    "    \n",
    "    # Path to store temporary jobfile used by worker node in pipeline execution\n",
    "    TEMP_LOCATION     = \"gs://myfirstbucket-1/tmp\"\n",
    "    \n",
    "    # Job name for this session\n",
    "    # * Used when gcp dataflow is used as runner\n",
    "    JOB_NAME          = \"job-write-file\"\n",
    "\n",
    "    # Project name in GCP. Needed when Data Flow Runner is used \n",
    "    # * Used when gcp dataflow is used as runner\n",
    "    PROJECT           = \"elite-caster-125113\"\n",
    "    \n",
    "    # File path of file to read from\n",
    "    #FILE_PATH        = \"/home/ywatanabe/notebook/sample.txt\"\n",
    "    FILE_PATH         = \"gs://ywatanabe/Documents/sample.txt\"\n",
    "    \n",
    "    # Region name where pipeline to operate\n",
    "    REGION            = \"asia-northeast1\"\n",
    "    \n",
    "    options= {\n",
    "        'project'          : PROJECT,\n",
    "        'runner'           : RUNNER,\n",
    "        'staging_location' : STAGING_LOCATION,\n",
    "        'temp_location'    : TEMP_LOCATION,\n",
    "        'job_name'         : JOB_NAME,\n",
    "        \"region\"           : REGION\n",
    "    }\n",
    "\n",
    "    # Configure Pipeline by creating PiplineOptions() object \n",
    "    o = beam.pipeline.PipelineOptions( **options )\n",
    "    \n",
    "    # Build Pipeline\n",
    "    with beam.Pipeline( options = o ) as p:\n",
    "        logging.info('Pipeline created.')\n",
    "        # Create PCollection object\n",
    "        # Read lines from file\n",
    "        lines = p | 'ReadFromText' >> ReadFromText( FILE_PATH )\n",
    "    \n",
    "        debug = ( lines | 'Print' >> beam.Map(print_info) )\n",
    "    \n",
    "        # Just simply write out the lines\n",
    "        lines | 'WriteToText' >>  WriteToText(\n",
    "            FILE_PATH, file_name_suffix='-write'\n",
    "        )\n",
    "\n",
    "def main():\n",
    "    run_pipeline()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
