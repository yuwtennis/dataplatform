{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/usr/local/creds/GoogleDev-f20867bbe55e.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=/usr/local/creds/GoogleDev-f20867bbe55e.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pipeline created.\n",
      "INFO:root:Starting the size estimation of the input\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Finished the size estimation of the input at 1 files. Estimation took 0.417656183243 seconds\n",
      "INFO:root:Starting the size estimation of the input\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Finished the size estimation of the input at 1 files. Estimation took 0.407822132111 seconds\n",
      "INFO:root:Starting GCS upload to gs://myfirstbucket-1/staging/job-write-file.1528236186.932713/pipeline.pb...\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Completed GCS upload to gs://myfirstbucket-1/staging/job-write-file.1528236186.932713/pipeline.pb\n",
      "INFO:root:Staging the SDK tarball from PyPI to gs://myfirstbucket-1/staging/job-write-file.1528236186.932713/dataflow_python_sdk.tar\n",
      "INFO:root:Executing command: ['/opt/anaconda2/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpaqbsbW', 'google-cloud-dataflow==2.4.0', '--no-binary', ':all:', '--no-deps']\n",
      "INFO:root:file copy from /tmp/tmpaqbsbW/google-cloud-dataflow-2.4.0.tar.gz to gs://myfirstbucket-1/staging/job-write-file.1528236186.932713/dataflow_python_sdk.tar.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:root:Create job: <Job\n",
      " createTime: u'2018-06-05T22:03:18.190300Z'\n",
      " currentStateTime: u'1970-01-01T00:00:00Z'\n",
      " id: u'2018-06-05_15_03_16-4558947171686803163'\n",
      " location: u'asia-northeast1'\n",
      " name: u'job-write-file'\n",
      " projectId: u'elite-caster-125113'\n",
      " stageStates: []\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:root:Created job with id: [2018-06-05_15_03_16-4558947171686803163]\n",
      "INFO:root:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/asia-northeast1/jobs/2018-06-05_15_03_16-4558947171686803163?project=elite-caster-125113\n",
      "INFO:root:Job 2018-06-05_15_03_16-4558947171686803163 is in state JOB_STATE_PENDING\n",
      "INFO:root:2018-06-05T22:03:16.385Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2018-06-05_15_03_16-4558947171686803163. The number of workers will be between 1 and 1000.\n",
      "INFO:root:2018-06-05T22:03:16.389Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2018-06-05_15_03_16-4558947171686803163.\n",
      "INFO:root:2018-06-05T22:03:20.818Z: JOB_MESSAGE_DETAILED: Checking required Cloud APIs are enabled.\n",
      "INFO:root:2018-06-05T22:03:21.152Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:root:2018-06-05T22:03:22.434Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in asia-northeast1-b.\n",
      "INFO:root:2018-06-05T22:03:23.796Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:root:2018-06-05T22:03:23.804Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:root:2018-06-05T22:03:23.811Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:root:2018-06-05T22:03:23.818Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:root:2018-06-05T22:03:23.832Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:root:2018-06-05T22:03:23.844Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:root:2018-06-05T22:03:23.851Z: JOB_MESSAGE_DETAILED: Fusing consumer Print into Count\n",
      "INFO:root:2018-06-05T22:03:23.857Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Count\n",
      "INFO:root:2018-06-05T22:03:23.864Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into WriteToText/Write/WriteImpl/Pair\n",
      "INFO:root:2018-06-05T22:03:23.870Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/GroupByKey/Reify into WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:root:2018-06-05T22:03:23.877Z: JOB_MESSAGE_DETAILED: Fusing consumer Count into ReadFromText/Read\n",
      "INFO:root:2018-06-05T22:03:23.883Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/Pair into WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:root:2018-06-05T22:03:23.890Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/Extract into WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:root:2018-06-05T22:03:23.897Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:root:2018-06-05T22:03:23.903Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/GroupByKey/Write into WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:root:2018-06-05T22:03:23.910Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToText/Write/WriteImpl/InitializeWrite into WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:root:2018-06-05T22:03:23.918Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:root:2018-06-05T22:03:23.923Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:root:2018-06-05T22:03:23.929Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:root:2018-06-05T22:03:23.937Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:root:2018-06-05T22:03:23.982Z: JOB_MESSAGE_DEBUG: Executing wait step start15\n",
      "INFO:root:2018-06-05T22:03:23.998Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/DoOnce/Read+WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:root:2018-06-05T22:03:24.005Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:root:2018-06-05T22:03:24.016Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:root:2018-06-05T22:03:24.023Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-northeast1-b...\n",
      "INFO:root:2018-06-05T22:03:24.044Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:root:Job 2018-06-05_15_03_16-4558947171686803163 is in state JOB_STATE_RUNNING\n",
      "INFO:root:2018-06-05T22:03:34.359Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 0 based on the rate of progress in the currently running step(s).\n",
      "INFO:root:2018-06-05T22:03:46.110Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:root:2018-06-05T22:04:03.567Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:root:2018-06-05T22:04:59.695Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:root:2018-06-05T22:04:59.702Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:root:2018-06-05T22:04:59.717Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:root:2018-06-05T22:04:59.723Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:root:2018-06-05T22:04:59.872Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:root:2018-06-05T22:04:59.887Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:root:2018-06-05T22:04:59.895Z: JOB_MESSAGE_BASIC: Executing operation ReadFromText/Read+Count+Print+WriteToText/Write/WriteImpl/WriteBundles/WriteBundles+WriteToText/Write/WriteImpl/Pair+WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+WriteToText/Write/WriteImpl/GroupByKey/Reify+WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:root:2018-06-05T22:05:07.726Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:root:2018-06-05T22:05:07.743Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/GroupByKey/Read+WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+WriteToText/Write/WriteImpl/Extract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:2018-06-05T22:05:19.586Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:root:2018-06-05T22:05:19.602Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:root:2018-06-05T22:05:19.757Z: JOB_MESSAGE_DEBUG: Value \"WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:root:2018-06-05T22:05:19.773Z: JOB_MESSAGE_BASIC: Executing operation WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:root:2018-06-05T22:05:28.020Z: JOB_MESSAGE_DEBUG: Executing success step success13\n",
      "INFO:root:2018-06-05T22:05:28.841Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:root:2018-06-05T22:05:29.200Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:root:2018-06-05T22:05:29.208Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:root:2018-06-05T22:06:01.813Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:root:2018-06-05T22:06:01.821Z: JOB_MESSAGE_DETAILED: Autoscaling: Would further reduce the number of workers but reached the minimum number allowed for the job.\n",
      "INFO:root:2018-06-05T22:06:01.840Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:root:Job 2018-06-05_15_03_16-4558947171686803163 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "class ComputeWordLenFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        \n",
    "        return [len( element )]\n",
    "\n",
    "def print_info(s):\n",
    "    print s\n",
    "\n",
    "def run_pipeline():\n",
    "    # Prepare necessary options for beam pipeline\n",
    "    # https://beam.apache.org/documentation/runners/capability-matrix/\n",
    "    #\n",
    "    # Pipline parameters in detail\n",
    "    # https://cloud.google.com/dataflow/pipelines/specifying-exec-params\n",
    "    \n",
    "    # Specify the which runner to use\n",
    "    # RUNNER          = \"DirectRunner\"\n",
    "    RUNNER            = \"DataflowRunner\"\n",
    "    \n",
    "    # Path to store the code package used by worker in during staging\n",
    "    STAGING_LOCATION  = \"gs://myfirstbucket-1/staging\"\n",
    "    \n",
    "    # Path to store temporary jobfile used by worker node in pipeline execution\n",
    "    TEMP_LOCATION     = \"gs://myfirstbucket-1/tmp\"\n",
    "    \n",
    "    # Job name for this session\n",
    "    # * Used when gcp dataflow is used as runner\n",
    "    JOB_NAME          = \"job-write-file\"\n",
    "\n",
    "    # Project name in GCP. Needed when Data Flow Runner is used \n",
    "    # * Used when gcp dataflow is used as runner\n",
    "    PROJECT           = \"elite-caster-125113\"\n",
    "    \n",
    "    # File path of file to read from\n",
    "    #FILE_PATH        = \"/home/ywatanabe/notebook/sample.txt\"\n",
    "    FILE_PATH         = \"gs://ywatanabe/Documents/sample.txt\"\n",
    "    \n",
    "    # Region name where pipeline to operate\n",
    "    REGION            = \"asia-northeast1\"\n",
    "    \n",
    "    options= {\n",
    "        'project'          : PROJECT,\n",
    "        'runner'           : RUNNER,\n",
    "        'staging_location' : STAGING_LOCATION,\n",
    "        'temp_location'    : TEMP_LOCATION,\n",
    "        'job_name'         : JOB_NAME,\n",
    "        \"region\"           : REGION\n",
    "    }\n",
    "\n",
    "    # Configure Pipeline by creating PiplineOptions() object \n",
    "    o = beam.pipeline.PipelineOptions( **options )\n",
    "    \n",
    "    # Build Pipeline\n",
    "    with beam.Pipeline( options = o ) as p:\n",
    "        logging.info('Pipeline created.')\n",
    "        # Create PCollection object\n",
    "        # Read lines from file\n",
    "        lines = ( p | 'ReadFromText' >> ReadFromText( FILE_PATH ) \n",
    "                  | 'Count' >> beam.ParDo(ComputeWordLenFn()) )\n",
    "    \n",
    "        debug = ( lines | 'Print' >> beam.Map(print_info) )\n",
    "    \n",
    "        # Just simply write out the lines\n",
    "        lines | 'WriteToText' >>  WriteToText(\n",
    "            FILE_PATH, file_name_suffix='-write'\n",
    "        )\n",
    "\n",
    "def main():\n",
    "    run_pipeline()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
